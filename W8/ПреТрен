Pre-training -> use large amount of data without labels (train to perform language modelling onf a large text, then fine-tune)

Pre-training requirements:
  Large amount of data
  High Quality (free from noise, errors, low-quality web content, gramatically correct, accurate, factual)
  Diversity & Representativeness (cover multiple domains, wide range of writing styles/formats/perspectives, balanced perspectives across cultures, languages and demographics)

Pre-training 3 types of architecture:
  Encoders ->  Bi-Directional context, can condition on future, challenge: to build strong representations
  Encoder-Decoder -> encoder - benefits from bidirectional context, 
  Decoders -> language model, cant be conditioned on future words,

BERT -> Bidirectional Encoder Representations from Transformers:
  Masked Language Modelling -> do LM/similar tasks with a transformer encoder: Mask k% of input words, then predict the masked words (usually k = 15) (for 15% predicted words -> replace 80% with mask, 10% with random token, 10% with leave unchanged)

  Next Sentence Prediction -> understand relationship between two sentences, designed to reduce gap between pre-training and fine-tuning, 

  Pre-training: MLM & NSP - trained together, CLS - pre-trained for NSP, other representations trained for MLM
  Fine-tuning: 

  Extensions: RoBERTa -> trained longer, without NSP; SpanBERT -> masks contiguous spans of words for a more robust pre-training

  Pros: Considers both left and right context, captures intricate contextual relationships
  Cons: not suitable/good at generating open-text from left-to right, one token at a time


T5 -> use span corruption: replace different-length spans from the input with unique placeholders, decode out the spans that were removed (implemented in text pre-processing)

GPT -> Generative Pretrained Transformer 
